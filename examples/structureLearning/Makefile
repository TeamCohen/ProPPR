include ../Makefile.in

### DATASET
NAME=uml_1
TRAIN=${NAME}-train
TEST=${NAME}-test

SRW=l2p:mu=0.0001:eta=1.0
EPOCHS=100
PROVER=dpr
APR=eps=0.0001:alph=0.001

ifeq ($(strip $(ITERS)),)
ITERS=10
endif

### TARGETS
# To run fully, do
#     $ make isg
#     $ make

all: results.txt

include ../common.in

clean:
	rm -f *results.txt *.grounded *.solutions.txt params.*.wts *.gradient \
	$(foreach ruletype,h22 delta alone,$(addprefix *_$(ruletype)_*,.ppr .wam))

.PRECIOUS: %.examples params.wts %.solutions.txt

## Below adapted from 
## /remote/curtis/yww/data/forKatie/uml/Makefile
## and
## /remote/curtis/yww/data/forKatie/uml/bin/iterativeGradientFinder.py
## March 2015

# This project iteratively refines a program based on h22.ppr. At each
# iteration, the rulefile ${NAME}_h22_XX.ppr is formed by
# concatenating h22.ppr and those rules generated by the previous
# iteration.
#
# At iteration t, we perform (t-1) epochs of training, then take the
# gradient of the loss. Additional rules are generated based on
# features with a negative gradient. These additional rules are stored
# in ${NAME}_delta_$t.ppr, and we proceed until no new rules are added.
#
# We measure the performance after each iteration by evaluating
# ${NAME}_alone_$t.ppr -- which is formed only from
# ${NAME}_delta_$t.ppr and interp.ppr, and does not include the h22
# rules -- on the test queries.

# Iteration 0 is the baseline, without any additional rules.
#
# Iteration 1 is the SG method, which uses only one gradient
# calculation to compute rules.
#
# All other iterations form the ISG method, which iterates on the
# gradient until no new rules are added. The performance of ISG is the
# performance of the final iteration.

structureLearning: $(foreach it,$(wildcard ${NAME}_delta_*.ppr),$(addsuffix $(subst .ppr,.solutions.txt,$(subst _delta,-test.alone,$(it))),pre. post.))

results.txt: $(foreach it,$(wildcard ${NAME}_delta_*.ppr),$(addsuffix $(subst .ppr,.results.txt,$(subst _delta,-test.alone,$(it))),pre. post.))
	echo phase.subset.iteration uR mR uMRR mMRR uMAP mMAP > $@
	cat $^ >> $@

# make  e.g. pre.uml_1-test.alone_01.results.txt
# from  e.g. pre.uml_1-test.alone_01.solutions.txt
# using e.g.     uml_1-test.examples
%.results.txt: %.solutions.txt
	ROOT=$*;\
	ROOT=$${ROOT#*.};\
	ROOT=$${ROOT%.*};\
	python ${PROPPR}/scripts/answermetrics.py --data $${ROOT}.examples --answers $< --metric recall --metric mrr --metric map |\
	grep -e "micro:" -e "macro:" |\
	awk '{print $$3}' |\
	tr "\n" " " |\
	awk '{name="$*"; gsub("\."," ",name); print name,$$0}' > $@
	cat $@

## The common.in target definitions are insufficient for our needs,
## since the program will differ for each iteration. Thus we define
## our own solutions targets, which include the .wam program
## dependency for that iteration.

# make e.g. pre.uml_1-test.h22_01.solutions.txt
# from e.g.     uml_1-test.examples
#               uml_1_h22_01.wam
pre.${NAME}-test.%.solutions.txt: ${NAME}-test.examples ${NAME}_%.wam
	java ${JOPTS} -cp ${CP} edu.cmu.ml.proppr.QueryAnswerer --programFiles ${NAME}-test.cfacts:$(word 2,$^) --queries $< --solutions $@ --prover ${PROVER} --threads ${THREADS} --apr ${APR} ${UNNORMALIZED}


# make e.g. post.uml_1-test.h22_01.solutions.txt
# from e.g.      uml_1-test.examples
#                uml_1_h22_01.wam
#               params.h22_01.wts
post.${NAME}-test.%.solutions.txt: ${NAME}-test.examples ${NAME}_%.wam params.%.wts
	java ${JOPTS} -cp ${CP} edu.cmu.ml.proppr.QueryAnswerer --programFiles ${NAME}-test.cfacts:$(word 2,$^) --queries $< --solutions $@ --prover ${PROVER} --threads ${THREADS} --apr ${APR} ${UNNORMALIZED} --params $(word 3,$^)

# make e.g.      params.alone_01.wts
# from e.g. uml_1-train.alone_01.examples.grounded
params.%.wts: ${TRAIN}.%.examples.grounded
	java ${JOPTS} -cp ${CP} edu.cmu.ml.proppr.Trainer --train $< --params $@ --threads ${THREADS} --srw ${SRW} --epochs ${EPOCHS} --apr ${APR}

# make e.g. uml_1-train.h22_01.examples.grounded
# from e.g. uml_1-train.examples
#           uml_1_h22_01.wam
${NAME}-train.%.examples.grounded: ${NAME}-train.examples ${NAME}_%.wam
	java ${JOPTS} -cp ${CP} edu.cmu.ml.proppr.Grounder --programFiles ${NAME}-train.cfacts:$(word 2,$^) --queries $< --grounded $@ --prover ${PROVER} --threads ${THREADS} --apr ${APR}

## Additional targets for building an executable program from gradient-generated rules:

# uml_1_delta_01.ppr + h22.ppr = uml_1_h22_01.ppr
${NAME}_h22_%.ppr: ${NAME}_delta_%.ppr h22.ppr
	cat $^ > $@

# uml_1_delta_01.ppr + interp.ppr = uml_1_alone_01.ppr
${NAME}_alone_%.ppr: ${NAME}_delta_%.ppr interp.ppr
	cat $^ > $@

# Actually run the iterated structural gradient procedure, and supporting target:

isg:
	python scripts/iterativeGradientFinder.py ${NAME} ${ITERS}

# make e.g.           uml_1_01.gradient
# from e.g. uml_1-train.h22_01.examples.grounded
# first training for e.g.   01 epochs
${NAME}_%.gradient: ${NAME}-train.h22_%.examples.grounded
	java ${JOPTS} -cp ${CP} edu.cmu.ml.proppr.GradientFinder --grounded $< --threads ${THREADS} --apr ${APR} --epochs $* --srw ${SRW} --gradient $@